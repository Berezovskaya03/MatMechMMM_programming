После сравнения cvm и knn выяснили, что точность работы у них одинакова. 
Однако из интернета знаем, что различия существуют.

KNN
Плюсы
1.Реализовать очень просто
2.Не требует обучения перед тем, как делать прогнозы в реальном времени. Это делает алгоритм KNN намного быстрее, чем другие алгоритмы, требующие обучения, например SVM, линейная регрессия и т. Д.
3.Поскольку алгоритм не требует обучения перед выполнением прогнозов, новые данные можно добавлять без проблем.
4.Для реализации KNN требуются только два параметра, то есть значение K и функция расстояния (например, евклидова или манхэттенская и т. Д.)
Минусы
1.Алгоритм KNN плохо работает с данными большого размера, потому что при большом количестве измерений алгоритму становится сложно вычислять расстояние в каждом измерении.
2.Алгоритм KNN имеет высокую стоимость прогнозирования для больших наборов данных. Это связано с тем, что в больших наборах данных стоимость вычисления расстояния между новой точкой и каждой существующей точкой становится выше.
3.Наконец, алгоритм KNN плохо работает с категориальными признаками, поскольку трудно найти расстояние между измерениями с категориальными признаками.

Плюсы и минусы классического SVM:
Плюсы:
1.Хорошо работает с пространством признаков большого размера;
2.Хорошо работает с данными небольшого объема;
3.Алгоритм максимизирует разделяющую полосу, которая, как подушка безопасности, позволяет уменьшить количество ошибок классификации;
4.Так как алгоритм сводится к решению задачи квадратичного программирования в выпуклой области, то такая задача всегда имеет единственное решение (разделяющая гиперплоскость с определенными гиперпараметрами алгоритма всегда одна).
Минусы:
1.Долгое время обучения (для больших наборов данных);
2.Неустойчивость к шуму: выбросы в обучающих данных становятся опорными объектами-нарушителями и напрямую влияют на построение разделяющей гиперплоскости;
3.Не описаны общие методы построения ядер и спрямляющих пространств, наиболее подходящих для конкретной задачи в случае линейной неразделимости классов. Подбирать полезные преобразования данных – искусство.

Отсутствие видимых различий связано, как я думаю с малым количеством данных.
